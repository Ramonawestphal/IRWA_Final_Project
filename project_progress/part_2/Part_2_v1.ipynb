{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8f2a97-c6a0-4607-b5b5-e6aed8dc661f",
   "metadata": {},
   "source": [
    "**Imports , NLKT Setup and Query Tokenizer (From Part 1)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68129a0e-1dda-4f4c-9bd7-4d54ead9841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import re, unicodedata\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# --- NLTK Components for Tokenization ---\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "# Asume que NLTK ya está instalado y descargado (como en tu Notebook de Parte 1)\n",
    "\n",
    "_STEM = PorterStemmer()\n",
    "_STOP = set(stopwords.words(\"english\"))\n",
    "_PUNCT = re.compile(r\"[^\\w\\s]+\", re.UNICODE)\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"Normalization utility from Part 1 (used for categorical/numeric fields).\"\"\"\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    s = re.sub(r\"[^\\w\\s]+\", \" \", s)\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def build_terms(text: str) -> list[str]:\n",
    "    \"\"\"Applies Part 1 preprocessing to the query (tokenize, stem, filter).\"\"\"\n",
    "    if not isinstance(text, str): return []\n",
    "    s = unicodedata.normalize(\"NFKC\", text.lower())\n",
    "    s = _PUNCT.sub(\" \", s)\n",
    "    toks = [t for t in s.split() if t not in _STOP]\n",
    "    toks = [_STEM.stem(t) for t in toks]\n",
    "    # Retiene caracteres individuales no numéricos (fix para H&M)\n",
    "    return [t for t in toks if not t.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc410b6b-791c-4d9a-972f-90aa4e3b3a10",
   "metadata": {},
   "source": [
    "**Inverted Index Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7678a05d-48e2-49e6-88fc-7695a826102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"Stores DF, Posting List (pid, tf), and Document Length (L_d).\"\"\"\n",
    "    def __init__(self):\n",
    "        # Index: { term: { 'df': int, 'postings': { pid: tf } } }\n",
    "        self.index = defaultdict(lambda: {'df': 0, 'postings': {}})\n",
    "        # L_d: { pid: L_d } (Euclidean norm for VSM/Cosine Similarity)\n",
    "        self.doc_lengths = {}\n",
    "        self.num_docs = 0\n",
    "\n",
    "    def add_document(self, doc_id: str, tokens: list[str]):\n",
    "        \"\"\"Calculates TF and L_d, and adds terms to postings.\"\"\"\n",
    "        tf_counts = Counter(tokens)\n",
    "        \n",
    "        # L_d (Euclidean norm of the raw term vector)\n",
    "        L_d = math.sqrt(sum(tf_counts[term]**2 for term in tf_counts))\n",
    "        self.doc_lengths[doc_id] = L_d\n",
    "        \n",
    "        # Update postings list and document frequency (df)\n",
    "        for term, tf in tf_counts.items():\n",
    "            if doc_id not in self.index[term]['postings']:\n",
    "                self.index[term]['df'] += 1\n",
    "            self.index[term]['postings'][doc_id] = tf\n",
    "        \n",
    "        self.num_docs += 1\n",
    "\n",
    "    def build_from_dataframe(self, df: pd.DataFrame):\n",
    "        \"\"\"Builds index from Part 1 processed data.\"\"\"\n",
    "        self.num_docs = len(df)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            pid = row['pid']\n",
    "            # Accessing the pre-processed token columns\n",
    "            tokens = row['title_tokens'] + row['desc_tokens'] + row.get('details_tokens', [])\n",
    "            self.add_document(pid, tokens)\n",
    "        \n",
    "        print(f\"Index built with {self.num_docs} documents and {len(self.index)} terms.\")\n",
    "    \n",
    "    def get_term_stats(self, term):\n",
    "        \"\"\"Retrieves statistics for a given term.\"\"\"\n",
    "        return self.index.get(term, {'df': 0, 'postings': {}})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55445a-ef75-4d64-acdf-a604ee20fd5d",
   "metadata": {},
   "source": [
    "**TF-IDF Ranking and Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365ea067-152e-4163-baf6-0a67b534ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_weight(tf, df, N):\n",
    "    \"\"\"Calculates the W_t,d or W_t,q TF-IDF weight.\"\"\"\n",
    "    # TF Component: 1 + log(tf) (Log-frequency weighting)\n",
    "    tf_comp = 1 + math.log10(tf) if tf > 0 else 0\n",
    "    # IDF Component: log(N/df)\n",
    "    idf_comp = math.log10(N / df) if df > 0 else 0\n",
    "    return tf_comp * idf_comp\n",
    "\n",
    "def ranked_search(query: str, index: InvertedIndex) -> list[tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Retrieves documents using strict AND logic and ranks them using Cosine Similarity.\n",
    "    \"\"\"\n",
    "    N = index.num_docs\n",
    "    # Use the existing tokenizer for query\n",
    "    query_tokens = build_terms(query) \n",
    "    if not query_tokens: return []\n",
    "    \n",
    "    # --- 1. Retrieval (Conjunctive AND Logic) ---\n",
    "    doc_sets = []\n",
    "    for term in query_tokens:\n",
    "        postings = index.get_term_stats(term)['postings']\n",
    "        if not postings:\n",
    "            return [] # Empty result if any term is missing\n",
    "        doc_sets.append(set(postings.keys()))\n",
    "        \n",
    "    retrieved_pids = list(set.intersection(*doc_sets))\n",
    "    \n",
    "    # --- 2. Ranking (Vector Space Model: Cosine Similarity) ---\n",
    "    scores = defaultdict(float)\n",
    "    q_tf_counts = Counter(query_tokens)\n",
    "    \n",
    "    for pid in retrieved_pids:\n",
    "        score = 0\n",
    "        L_d = index.doc_lengths.get(pid, 1.0)\n",
    "        \n",
    "        # Calculate Dot Product: sum(W_t,q * W_t,d)\n",
    "        for term in query_tokens:\n",
    "            term_stats = index.get_term_stats(term)\n",
    "            df = term_stats['df']\n",
    "            \n",
    "            # W_t,d (Document weight)\n",
    "            tf_d = term_stats['postings'].get(pid, 0)\n",
    "            W_t_d = calculate_tfidf_weight(tf_d, df, N)\n",
    "            \n",
    "            # W_t,q (Query weight)\n",
    "            tf_q = q_tf_counts[term]\n",
    "            W_t_q = calculate_tfidf_weight(tf_q, df, N)\n",
    "\n",
    "            score += W_t_q * W_t_d\n",
    "            \n",
    "        # Cosine Similarity (Score = Dot Product / L_d)\n",
    "        scores[pid] = score / L_d\n",
    "        \n",
    "    # --- 3. Sort Results ---\n",
    "    return sorted(scores.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c948ce-6584-4b9b-9b9b-3a097b5049ff",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31818f0-cabb-4033-bfaf-805e9aa7346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_labels(query_id, retrieved_pids, df_labels):\n",
    "    \"\"\"Returns ranked binary relevance scores (1/0) and R_total.\"\"\"\n",
    "    relevant_labels_df = df_labels[df_labels['query_id'] == query_id]\n",
    "    relevant_pids = set(relevant_labels_df[relevant_labels_df['relevance'] == 1]['pid'])\n",
    "    R_total = len(relevant_pids)\n",
    "    relevance_scores = [1 if pid in relevant_pids else 0 for pid in retrieved_pids]\n",
    "    return relevance_scores, R_total\n",
    "\n",
    "# --- Standard Cutoff Metrics ---\n",
    "def precision_at_k(rel_scores, k):\n",
    "    \"\"\"P@K (Required i)\"\"\"\n",
    "    if k == 0 or not rel_scores: return 0.0\n",
    "    k = min(k, len(rel_scores))\n",
    "    return sum(rel_scores[:k]) / k\n",
    "\n",
    "def recall_at_k(rel_scores, R_total, k):\n",
    "    \"\"\"R@K (Required ii)\"\"\"\n",
    "    if R_total == 0: return 0.0\n",
    "    k = min(k, len(rel_scores))\n",
    "    return sum(rel_scores[:k]) / R_total\n",
    "\n",
    "def f1_score_at_k(P_at_k, R_at_k):\n",
    "    \"\"\"F1-Score@K (Required iv)\"\"\"\n",
    "    if P_at_k + R_at_k == 0: return 0.0\n",
    "    return 2 * P_at_k * R_at_k / (P_at_k + R_at_k)\n",
    "\n",
    "# --- Ranking Metrics ---\n",
    "def average_precision_at_k(rel_scores, k):\n",
    "    \"\"\"AP@K (Required iii)\"\"\"\n",
    "    if not rel_scores: return 0.0\n",
    "    k = min(k, len(rel_scores))\n",
    "    sum_of_precisions = 0.0\n",
    "    num_relevant = 0\n",
    "    for i in range(k):\n",
    "        if rel_scores[i] == 1:\n",
    "            num_relevant += 1\n",
    "            sum_of_precisions += num_relevant / (i + 1)\n",
    "    return sum_of_precisions / num_relevant if num_relevant > 0 else 0.0\n",
    "\n",
    "def mean_average_precision(aps_list):\n",
    "    \"\"\"MAP (Required v)\"\"\"\n",
    "    return sum(aps_list) / len(aps_list) if aps_list else 0.0\n",
    "\n",
    "def mean_reciprocal_rank(rel_scores: list):\n",
    "    \"\"\"MRR (Required vi)\"\"\"\n",
    "    for i, rel in enumerate(rel_scores):\n",
    "        if rel == 1:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(rel_scores, k):\n",
    "    \"\"\"NDCG@K (Required vii)\"\"\"\n",
    "    k = min(k, len(rel_scores))\n",
    "    \n",
    "    # DCG (Actual Ranking)\n",
    "    dcg = sum(rel_scores[i] / math.log2(i + 2) for i in range(k))\n",
    "    \n",
    "    # IDCG (Ideal Ranking)\n",
    "    ideal_scores = sorted(rel_scores, reverse=True)\n",
    "    idcg = sum(ideal_scores[i] / math.log2(i + 2) for i in range(k))\n",
    "        \n",
    "    return dcg / idcg if idcg > 0.0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6129b-2b5f-47a7-b193-f9bf60b1d839",
   "metadata": {},
   "source": [
    "**Execution - Build Index and Define Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f186a20-6376-465a-a102-cc4c8ddda89d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/raw/validation_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(INDEX_FILE), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 1. Load ground truth labels\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(LABELS_FILE)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --- B. Index Construction ---\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- 1. BUILDING INVERTED INDEX ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/raw/validation_labels.csv'"
     ]
    }
   ],
   "source": [
    "# --- A. Setup ---\n",
    "INDEX_FILE = 'data/index/inverted_index.json'\n",
    "PROCESSED_DATA_FILE = 'data/processed/products_clean.parquet'\n",
    "LABELS_FILE = 'data/raw/validation_labels.csv'\n",
    "\n",
    "os.makedirs(os.path.dirname(INDEX_FILE), exist_ok=True)\n",
    "\n",
    "# 1. Load ground truth labels\n",
    "df_labels = pd.read_csv(LABELS_FILE)\n",
    "\n",
    "# --- B. Index Construction ---\n",
    "print(\"--- 1. BUILDING INVERTED INDEX ---\")\n",
    "\n",
    "# Check for processed file existence \n",
    "if not os.path.exists(PROCESSED_DATA_FILE):\n",
    "    print(f\"Error: Processed data not found at {PROCESSED_DATA_FILE}. Run Part 1 script.\")\n",
    "    sys.exit()\n",
    "\n",
    "df_clean = pd.read_parquet(PROCESSED_DATA_FILE)\n",
    "\n",
    "INDEX = InvertedIndex()\n",
    "INDEX.build_from_dataframe(df_clean)\n",
    "\n",
    "# Save index \n",
    "with open(INDEX_FILE, 'w') as f:\n",
    "    json.dump({'index': dict(INDEX.index), 'doc_lengths': dict(INDEX.doc_lengths)}, f)\n",
    "\n",
    "print(f\"Index saved to {INDEX_FILE}. Documents: {INDEX.num_docs}\")\n",
    "\n",
    "# --- C. Query Definitions (Rubric: Propose test queries - 1 point) ---\n",
    "QUERY_1 = \"women full sleeve sweatshirt cotton\"\n",
    "QUERY_2 = \"men slim jeans blue\"\n",
    "\n",
    "# New test queries (5 new queries)\n",
    "NEW_QUERIES_LIST = [\n",
    "    {\"query_id\": 3, \"query\": \"long sleeve denim jacket blue\"}, \n",
    "    {\"query_id\": 4, \"query\": \"reeb shoe sport white\"},       \n",
    "    {\"query_id\": 5, \"query\": \"cheap men polo shirt black\"},   \n",
    "    {\"query_id\": 6, \"query\": \"tight fit short skirt women\"},  \n",
    "    {\"query_id\": 7, \"query\": \"low price formal trouser\"},     \n",
    "]\n",
    "ALL_QUERIES = [\n",
    "    {'query_id': 1, 'query': QUERY_1},\n",
    "    {'query_id': 2, 'query': QUERY_2}\n",
    "] + NEW_QUERIES_LIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc67f28-3268-49d7-b24f-e67efef17373",
   "metadata": {},
   "source": [
    "**Evaluation 1 & 2 Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b503d69-eba3-4a1a-8faf-76aa23d08888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. RUNNING RANKED SEARCH AND EVALUATION ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ALL_QUERIES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m RESULTS \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m AP_SCORES \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q_info \u001b[38;5;129;01min\u001b[39;00m ALL_QUERIES:\n\u001b[0;32m      7\u001b[0m     query_id \u001b[38;5;241m=\u001b[39m q_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m     query_text \u001b[38;5;241m=\u001b[39m q_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ALL_QUERIES' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. RUNNING RANKED SEARCH AND EVALUATION ---\")\n",
    "\n",
    "RESULTS = []\n",
    "AP_SCORES = [] \n",
    "\n",
    "for q_info in ALL_QUERIES:\n",
    "    query_id = q_info['query_id']\n",
    "    query_text = q_info['query']\n",
    "    \n",
    "    # Run the TF-IDF Ranked Search (AND retrieval)\n",
    "    ranked_pids_scores = ranked_search(query_text, INDEX)\n",
    "    retrieved_pids = [pid for pid, score in ranked_pids_scores]\n",
    "\n",
    "    # --- Get Relevance Labels ---\n",
    "    relevance_scores, R_total = get_relevance_labels(query_id, retrieved_pids, df_labels)\n",
    "\n",
    "    # --- Calculate Metrics (Cutoff K=10) ---\n",
    "    k = 10 \n",
    "    \n",
    "    P_k = precision_at_k(relevance_scores, k)\n",
    "    R_k = recall_at_k(relevance_scores, R_total, k)\n",
    "    F1_k = f1_score_at_k(P_k, R_k)\n",
    "    AP_k = average_precision_at_k(relevance_scores, k)\n",
    "    MRR_score = mean_reciprocal_rank(relevance_scores)\n",
    "    NDCG_k = ndcg_at_k(relevance_scores, k)\n",
    "    \n",
    "    AP_SCORES.append(AP_k)\n",
    "    \n",
    "    # Collect results\n",
    "    RESULTS.append({\n",
    "        'Query ID': query_id,\n",
    "        'Query Text': query_text,\n",
    "        'R_Total': R_total,\n",
    "        'Retrieved': len(retrieved_pids),\n",
    "        'P@10': round(P_k, 3),\n",
    "        'R@10': round(R_k, 3),\n",
    "        'F1@10': round(F1_k, 3),\n",
    "        'AP@10': round(AP_k, 3),\n",
    "        'MRR': round(MRR_score, 3),\n",
    "        'NDCG@10': round(NDCG_k, 3),\n",
    "    })\n",
    "\n",
    "# --- D. Final Results and MAP Calculation ---\n",
    "df_results = pd.DataFrame(RESULTS)\n",
    "MAP_score = mean_average_precision(AP_SCORES)\n",
    "\n",
    "print(\"\\n--- 3. FINAL EVALUATION METRICS (Rounded to 3 decimals) ---\")\n",
    "# The 'display' function is often used in Jupyter Notebooks\n",
    "# If run outside, uncomment: print(df_results)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_results)\n",
    "except ImportError:\n",
    "    print(df_results)\n",
    "\n",
    "print(f\"\\nMean Average Precision (MAP) across all {len(ALL_QUERIES)} queries: {round(MAP_score, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05322ca-33ba-45d0-9741-8c10da32df65",
   "metadata": {},
   "source": [
    "**Ground Truth for New Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bd4f48e-33e8-498a-83eb-339c20d00770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The assignment requires you to manually define the ground truth for Q3-Q7 \n",
    "# and update your validation_labels.csv file. \n",
    "\n",
    "# ACTION REQUIRED:\n",
    "# 1. Inspect the PIDs retrieved by the search engine for Queries 3 through 7.\n",
    "# 2. For those PIDs, manually judge relevance (1 or 0).\n",
    "# 3. Add these new relevance judgments to your data/raw/validation_labels.csv file.\n",
    "# 4. Include a detailed table of your manual judgments in your final PDF report.\n",
    "\n",
    "# Example:\n",
    "# Query 3: 'long sleeve denim jacket blue'\n",
    "# - PID_12345: Relevant (1) because it is a denim jacket.\n",
    "# - PID_67890: Not Relevant (0) because it is a denim dress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80dde80-7f03-4fd0-aee0-a30424a66ce1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
