{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8f2a97-c6a0-4607-b5b5-e6aed8dc661f",
   "metadata": {},
   "source": [
    "**Imports , NLKT Setup and Query Tokenizer (From Part 1)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68129a0e-1dda-4f4c-9bd7-4d54ead9841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import re, unicodedata\n",
    "import os\n",
    "import sys \n",
    "import pathlib\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# NLTK Setup\n",
    "try:\n",
    "    stopwords.words(\"english\")\n",
    "except LookupError:\n",
    "    import nltk; nltk.download(\"stopwords\")\n",
    "# End NLTK Setup\n",
    "\n",
    "_STEM = PorterStemmer()\n",
    "_STOP = set(stopwords.words(\"english\"))\n",
    "_PUNCT = re.compile(r\"[^\\w\\s]+\", re.UNICODE)\n",
    "\n",
    "def build_terms(text: str) -> List[str]:\n",
    "    \"\"\"Applies the Part 1 tokenization, stemming, and filtering to a raw query.\"\"\"\n",
    "    if not isinstance(text, str): return []\n",
    "    s = unicodedata.normalize(\"NFKC\", text.lower())\n",
    "    s = _PUNCT.sub(\" \", s)\n",
    "    toks = [t for t in s.split() if t not in _STOP]\n",
    "    toks = [_STEM.stem(t) for t in toks]\n",
    "    # Filtro len(t) > 1 consistente con el uploaded:tokenize.py de Part 1.\n",
    "    return [t for t in toks if len(t) > 1 and not t.isdigit()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc410b6b-791c-4d9a-972f-90aa4e3b3a10",
   "metadata": {},
   "source": [
    "**Inverted Index Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7678a05d-48e2-49e6-88fc-7695a826102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex:\n",
    "    \"\"\"Stores DF, Posting List (pid, tf), and Document Length (L_d) for TF-IDF.\"\"\"\n",
    "    def __init__(self):\n",
    "        # Index: { term: { 'df': int, 'postings': { pid: tf } } }\n",
    "        self.index: Dict[str, Dict] = defaultdict(lambda: {'df': 0, 'postings': {}})\n",
    "        # L_d: { pid: L_d } (Euclidean norm for Cosine Similarity normalization)\n",
    "        self.doc_lengths: Dict[str, float] = {}\n",
    "        self.num_docs: int = 0\n",
    "\n",
    "    def add_document(self, doc_id: str, tokens: List[str]):\n",
    "        \"\"\"Calculates TF and L_d, and adds terms to postings.\"\"\"\n",
    "        tf_counts = Counter(tokens)\n",
    "        \n",
    "        # Calculate L_d (Euclidean norm of the raw term frequency vector)\n",
    "        L_d = math.sqrt(sum(tf_counts[term]**2 for term in tf_counts))\n",
    "        self.doc_lengths[doc_id] = L_d\n",
    "        \n",
    "        # Update postings list and document frequency (df)\n",
    "        for term, tf in tf_counts.items():\n",
    "            if doc_id not in self.index[term]['postings']:\n",
    "                self.index[term]['df'] += 1\n",
    "            self.index[term]['postings'][doc_id] = tf\n",
    "        \n",
    "        self.num_docs += 1\n",
    "\n",
    "    def build_from_dataframe(self, df: pd.DataFrame):\n",
    "        \"\"\"Builds index from Part 1 processed data.\"\"\"\n",
    "        self.num_docs = len(df)\n",
    "        \n",
    "        for index, row in df.iterrows():\n",
    "            pid = row['pid']\n",
    "            # Accessing the pre-processed token columns\n",
    "            tokens = row['title_tokens'] + row['desc_tokens'] + row.get('details_tokens', [])\n",
    "            self.add_document(pid, tokens)\n",
    "        \n",
    "        print(f\"Index built with {self.num_docs} documents and {len(self.index)} terms.\")\n",
    "    \n",
    "    def get_term_stats(self, term):\n",
    "        \"\"\"Retrieves statistics for a given term.\"\"\"\n",
    "        return self.index.get(term, {'df': 0, 'postings': {}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55445a-ef75-4d64-acdf-a604ee20fd5d",
   "metadata": {},
   "source": [
    "**TF-IDF Ranking and Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "365ea067-152e-4163-baf6-0a67b534ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tfidf_weight(tf, df, N):\n",
    "    \"\"\"Calculates the W_t,d or W_t,q TF-IDF weight using 1+log(tf) * log(N/df).\"\"\"\n",
    "    tf_comp = 1 + math.log10(tf) if tf > 0 else 0\n",
    "    idf_comp = math.log10(N / df) if df > 0 else 0\n",
    "    return tf_comp * idf_comp\n",
    "\n",
    "def ranked_search(query: str, index: InvertedIndex) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Retrieves documents using strict AND logic and ranks them using Cosine Similarity.\n",
    "    \"\"\"\n",
    "    N = index.num_docs\n",
    "    query_tokens = build_terms(query) \n",
    "    if not query_tokens: return []\n",
    "    \n",
    "    # Retrieval (Conjunctive AND Logic)\n",
    "    doc_sets = [set(index.get_term_stats(term)['postings'].keys()) for term in query_tokens]\n",
    "    \n",
    "    # Strict AND intersection: every document must contain every query term\n",
    "    retrieved_pids = list(set.intersection(*doc_sets))\n",
    "    \n",
    "    # If intersection is empty, or any query term was not found in the index\n",
    "    if not retrieved_pids or any(not index.get_term_stats(term)['df'] for term in query_tokens):\n",
    "        return []\n",
    "    \n",
    "    # Ranking (Vector Space Model: Cosine Similarity)\n",
    "    scores = defaultdict(float)\n",
    "    q_tf_counts = Counter(query_tokens)\n",
    "    \n",
    "    for pid in retrieved_pids:\n",
    "        score = 0\n",
    "        L_d = index.doc_lengths.get(pid, 1.0)\n",
    "        \n",
    "        # Calculate Dot Product: sum(W_t,q * W_t,d)\n",
    "        for term in query_tokens:\n",
    "            term_stats = index.get_term_stats(term)\n",
    "            df = term_stats['df']\n",
    "            \n",
    "            # W_t,d (Document weight)\n",
    "            tf_d = term_stats['postings'].get(pid, 0)\n",
    "            W_t_d = calculate_tfidf_weight(tf_d, df, N)\n",
    "            \n",
    "            # W_t,q (Query weight)\n",
    "            tf_q = q_tf_counts[term]\n",
    "            W_t_q = calculate_tfidf_weight(tf_q, df, N)\n",
    "\n",
    "            score += W_t_q * W_t_d\n",
    "            \n",
    "        # Cosine Similarity (Score = Dot Product / L_d)\n",
    "        scores[pid] = score / L_d\n",
    "        \n",
    "    # Sort Results\n",
    "    return sorted(scores.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c948ce-6584-4b9b-9b9b-3a097b5049ff",
   "metadata": {},
   "source": [
    "**Evaluation Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f31818f0-cabb-4033-bfaf-805e9aa7346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_labels(query_id, retrieved_pids, df_labels):\n",
    "    \"\"\"Returns ranked binary relevance scores (1/0) and R_total.\"\"\"\n",
    "    relevant_labels_df = df_labels[df_labels['query_id'] == query_id]\n",
    "    relevant_pids = set(relevant_labels_df[relevant_labels_df['relevance'] == 1]['pid'])\n",
    "    R_total = len(relevant_pids)\n",
    "    relevance_scores = [1 if pid in relevant_pids else 0 for pid in retrieved_pids]\n",
    "    return relevance_scores, R_total\n",
    "\n",
    "# Required Metrics\n",
    "def precision_at_k(rel_scores, k):\n",
    "    \"\"\"P@K (Required i)\"\"\"\n",
    "    if k == 0 or not rel_scores: return 0.0\n",
    "    k = min(k, len(rel_scores))\n",
    "    return sum(rel_scores[:k]) / k\n",
    "\n",
    "def recall_at_k(rel_scores, R_total, k):\n",
    "    \"\"\"R@K (Required ii)\"\"\"\n",
    "    if R_total == 0: return 0.0\n",
    "    k = min(k, len(rel_scores))\n",
    "    return sum(rel_scores[:k]) / R_total\n",
    "\n",
    "def f1_score_at_k(P_at_k, R_at_k):\n",
    "    \"\"\"F1-Score@K (Required iv)\"\"\"\n",
    "    if P_at_k + R_at_k == 0: return 0.0\n",
    "    return 2 * P_at_k * R_at_k / (P_at_k + R_at_k)\n",
    "\n",
    "def average_precision_at_k(rel_scores, k):\n",
    "    \"\"\"AP@K (Required iii)\"\"\"\n",
    "    if not rel_scores: return 0.0\n",
    "    k = min(k, len(rel_scores))\n",
    "    sum_of_precisions = 0.0\n",
    "    num_relevant = 0\n",
    "    for i in range(k):\n",
    "        if rel_scores[i] == 1:\n",
    "            num_relevant += 1\n",
    "            sum_of_precisions += num_relevant / (i + 1)\n",
    "    return sum_of_precisions / num_relevant if num_relevant > 0 else 0.0\n",
    "\n",
    "def mean_average_precision(aps_list):\n",
    "    \"\"\"MAP (Required v)\"\"\"\n",
    "    return sum(aps_list) / len(aps_list) if aps_list else 0.0\n",
    "\n",
    "def mean_reciprocal_rank(rel_scores: list):\n",
    "    \"\"\"MRR (Required vi)\"\"\"\n",
    "    for i, rel in enumerate(rel_scores):\n",
    "        if rel == 1:\n",
    "            return 1.0 / (i + 1)\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(rel_scores, k):\n",
    "    \"\"\"NDCG@K (Required vii)\"\"\"\n",
    "    k = min(k, len(rel_scores))\n",
    "    \n",
    "    # DCG (Actual Ranking)\n",
    "    dcg = sum(rel_scores[i] / math.log2(i + 2) for i in range(k))\n",
    "    \n",
    "    # IDCG (Ideal Ranking)\n",
    "    ideal_scores = sorted(rel_scores, reverse=True)\n",
    "    idcg = sum(ideal_scores[i] / math.log2(i + 2) for i in range(k))\n",
    "        \n",
    "    return dcg / idcg if idcg > 0.0 else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6129b-2b5f-47a7-b193-f9bf60b1d839",
   "metadata": {},
   "source": [
    "**Execution - Build Index and Define Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f186a20-6376-465a-a102-cc4c8ddda89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FATAL ERROR: Validation labels not found at C:\\Users\\david\\IRWA_Definitivo\\data\\raw\\validation_labels.csv. Ensure the file is in 'data/raw/'.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Safely determine the project root path\n",
    "current_dir = pathlib.Path(os.getcwd())\n",
    "\n",
    "try:\n",
    "    # Find the project root by looking for the 'data' directory upwards\n",
    "    PROJECT_ROOT = next(p for p in [current_dir] + list(current_dir.parents) if (p / 'data').exists())\n",
    "except StopIteration:\n",
    "    print(\"FATAL ERROR: Could not find project root containing the 'data' directory. Ensure notebook is run inside project structure.\")\n",
    "    sys.exit()\n",
    "    \n",
    "INDEX_FILE = PROJECT_ROOT / 'data' / 'index' / 'inverted_index.json'\n",
    "PROCESSED_DATA_FILE = PROJECT_ROOT / 'data' / 'processed' / 'products_clean.parquet'\n",
    "LABELS_FILE = PROJECT_ROOT / 'data' / 'raw' / 'validation_labels.csv'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "try:\n",
    "    os.makedirs(INDEX_FILE.parent, exist_ok=True)\n",
    "except PermissionError:\n",
    "    print(f\"PERMISSION ERROR: Failed to create index directory at {INDEX_FILE.parent}. Create 'data/index' manually.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Load ground truth labels\n",
    "if not LABELS_FILE.exists():\n",
    "    print(f\"FATAL ERROR: Validation labels not found at {LABELS_FILE}. Ensure the file is in 'data/raw/'.\")\n",
    "    sys.exit()\n",
    "\n",
    "df_labels = pd.read_csv(LABELS_FILE)\n",
    "\n",
    "# Index Construction\n",
    "print(\"--- 1. BUILDING INVERTED INDEX ---\")\n",
    "\n",
    "if not PROCESSED_DATA_FILE.exists():\n",
    "    print(f\"Error: Processed data not found at {PROCESSED_DATA_FILE}. Run Part 1 script.\")\n",
    "    sys.exit()\n",
    "\n",
    "#df_clean = pd.read_parquet(PROCESSED_DATA_FILE)\n",
    "dataset = ds.dataset(PROCESSED_DATA_FILE, format=\"parquet\")\n",
    "df_clean = dataset.to_table().to_pandas()\n",
    "\n",
    "INDEX = InvertedIndex()\n",
    "INDEX.build_from_dataframe(df_clean)\n",
    "\n",
    "# Save index \n",
    "with open(INDEX_FILE, 'w') as f:\n",
    "    json.dump({'index': dict(INDEX.index), 'doc_lengths': dict(INDEX.doc_lengths)}, f)\n",
    "\n",
    "print(f\"Index saved to {INDEX_FILE}. Documents: {INDEX.num_docs}\")\n",
    "\n",
    "# Query Definitions \n",
    "QUERY_1 = \"women full sleeve sweatshirt cotton\"\n",
    "QUERY_2 = \"men slim jeans blue\"\n",
    "\n",
    "# New test queries (5 new queries)\n",
    "NEW_QUERIES_LIST = [\n",
    "    {\"query_id\": 3, \"query\": \"long sleeve denim jacket blue\"}, \n",
    "    {\"query_id\": 4, \"query\": \"reeb shoe sport white\"},       \n",
    "    {\"query_id\": 5, \"query\": \"cheap men polo shirt black\"},   \n",
    "    {\"query_id\": 6, \"query\": \"tight fit short skirt women\"},  \n",
    "    {\"query_id\": 7, \"query\": \"low price formal trouser\"},     \n",
    "]\n",
    "ALL_QUERIES = [\n",
    "    {'query_id': 1, 'query': QUERY_1},\n",
    "    {'query_id': 2, 'query': QUERY_2}\n",
    "] + NEW_QUERIES_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc488e-1617-4698-a268-c8afd05ec247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder = r\"C:\\Users\\david\\IRWA_Definitivo\\IRWA_Final_Project\\project_progress\\part_2\\data\\processed\"\n",
    "for f in os.listdir(folder):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc67f28-3268-49d7-b24f-e67efef17373",
   "metadata": {},
   "source": [
    "**Evaluation 1 & 2 Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b503d69-eba3-4a1a-8faf-76aa23d08888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 2. RUNNING RANKED SEARCH AND EVALUATION ---\")\n",
    "\n",
    "RESULTS = []\n",
    "AP_SCORES = [] \n",
    "\n",
    "for q_info in ALL_QUERIES:\n",
    "    query_id = q_info['query_id']\n",
    "    query_text = q_info['query']\n",
    "    \n",
    "    # Run the TF-IDF Ranked Search (AND retrieval)\n",
    "    ranked_pids_scores = ranked_search(query_text, INDEX)\n",
    "    retrieved_pids = [pid for pid, score in ranked_pids_scores]\n",
    "\n",
    "    # --- Get Relevance Labels ---\n",
    "    relevance_scores, R_total = get_relevance_labels(query_id, retrieved_pids, df_labels)\n",
    "\n",
    "    # --- Calculate Metrics (Cutoff K=10) ---\n",
    "    k = 10 \n",
    "    \n",
    "    P_k = precision_at_k(relevance_scores, k)\n",
    "    R_k = recall_at_k(relevance_scores, R_total, k)\n",
    "    F1_k = f1_score_at_k(P_k, R_k)\n",
    "    AP_k = average_precision_at_k(relevance_scores, k)\n",
    "    MRR_score = mean_reciprocal_rank(relevance_scores)\n",
    "    NDCG_k = ndcg_at_k(relevance_scores, k)\n",
    "    \n",
    "    AP_SCORES.append(AP_k)\n",
    "    \n",
    "    # Collect results\n",
    "    RESULTS.append({\n",
    "        'Query ID': query_id,\n",
    "        'Query Text': query_text,\n",
    "        'R_Total': R_total,\n",
    "        'Retrieved': len(retrieved_pids),\n",
    "        'P@10': round(P_k, 3),\n",
    "        'R@10': round(R_k, 3),\n",
    "        'F1@10': round(F1_k, 3),\n",
    "        'AP@10': round(AP_k, 3),\n",
    "        'MRR': round(MRR_score, 3),\n",
    "        'NDCG@10': round(NDCG_k, 3),\n",
    "    })\n",
    "\n",
    "# --- D. Final Results and MAP Calculation ---\n",
    "df_results = pd.DataFrame(RESULTS)\n",
    "MAP_score = mean_average_precision(AP_SCORES)\n",
    "\n",
    "print(\"\\n--- 3. FINAL EVALUATION METRICS (Rounded to 3 decimals) ---\")\n",
    "# The 'display' function is often used in Jupyter Notebooks\n",
    "# If run outside, uncomment: print(df_results)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    display(df_results)\n",
    "except ImportError:\n",
    "    print(df_results)\n",
    "\n",
    "print(f\"\\nMean Average Precision (MAP) across all {len(ALL_QUERIES)} queries: {round(MAP_score, 3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05322ca-33ba-45d0-9741-8c10da32df65",
   "metadata": {},
   "source": [
    "**Ground Truth for New Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4f48e-33e8-498a-83eb-339c20d00770",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The assignment requires you to manually define the ground truth for Q3-Q7 \n",
    "# and update your validation_labels.csv file. \n",
    "\n",
    "# ACTION REQUIRED:\n",
    "# 1. Inspect the PIDs retrieved by the search engine for Queries 3 through 7.\n",
    "# 2. For those PIDs, manually judge relevance (1 or 0).\n",
    "# 3. Add these new relevance judgments to your data/raw/validation_labels.csv file.\n",
    "# 4. Include a detailed table of your manual judgments in your final PDF report.\n",
    "\n",
    "# Example:\n",
    "# Query 3: 'long sleeve denim jacket blue'\n",
    "# - PID_12345: Relevant (1) because it is a denim jacket.\n",
    "# - PID_67890: Not Relevant (0) because it is a denim dress.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
