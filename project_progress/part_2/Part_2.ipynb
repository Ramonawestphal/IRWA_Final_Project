{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "df8f2a97-c6a0-4607-b5b5-e6aed8dc661f",
      "metadata": {
        "id": "df8f2a97-c6a0-4607-b5b5-e6aed8dc661f"
      },
      "source": [
        "**Imports , NLKT Setup and Query Tokenizer (From Part 1)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "68129a0e-1dda-4f4c-9bd7-4d54ead9841e",
      "metadata": {
        "id": "68129a0e-1dda-4f4c-9bd7-4d54ead9841e",
        "outputId": "7dcc07c2-1661-4942-879f-6688683fa84d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "import re, unicodedata\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "# --- NLTK Setup ---\n",
        "try:\n",
        "    stopwords.words(\"english\")\n",
        "except LookupError:\n",
        "    import nltk\n",
        "    nltk.download(\"stopwords\")\n",
        "\n",
        "_STEM = PorterStemmer()\n",
        "_STOP = set(stopwords.words(\"english\"))\n",
        "_PUNCT = re.compile(r\"[^\\w\\s]+\", re.UNICODE)\n",
        "\n",
        "def build_terms(text: str) -> List[str]:\n",
        "    \"\"\"Applies tokenization, stemming, and filtering to raw query.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    s = unicodedata.normalize(\"NFKC\", text.lower())\n",
        "    s = _PUNCT.sub(\" \", s)\n",
        "    toks = [t for t in s.split() if t not in _STOP]\n",
        "    toks = [_STEM.stem(t) for t in toks]\n",
        "    return [t for t in toks if len(t) > 1 and not t.isdigit()]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc410b6b-791c-4d9a-972f-90aa4e3b3a10",
      "metadata": {
        "id": "cc410b6b-791c-4d9a-972f-90aa4e3b3a10"
      },
      "source": [
        "**Inverted Index Building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7678a05d-48e2-49e6-88fc-7695a826102f",
      "metadata": {
        "id": "7678a05d-48e2-49e6-88fc-7695a826102f"
      },
      "outputs": [],
      "source": [
        "class InvertedIndex:\n",
        "    \"\"\"Stores DF, Posting List (pid, tf), and Document Length (L_d) for TF-IDF.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.index: Dict[str, Dict] = defaultdict(lambda: {'df': 0, 'postings': {}})\n",
        "        self.doc_lengths: Dict[str, float] = {}\n",
        "        self.num_docs: int = 0\n",
        "\n",
        "    def build_from_dataframe(self, df: pd.DataFrame):\n",
        "        \"\"\"Builds index from Part 1 processed data.\"\"\"\n",
        "        print(f\"Building index from {len(df)} documents...\")\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            if idx % 1000 == 0:\n",
        "                print(f\"  Processed {idx}/{len(df)} documents...\")\n",
        "\n",
        "            pid = str(row['pid'])\n",
        "\n",
        "            # Combine all token columns\n",
        "            tokens = []\n",
        "            if 'title_tokens' in row and isinstance(row['title_tokens'], list):\n",
        "                tokens.extend(row['title_tokens'])\n",
        "            if 'desc_tokens' in row and isinstance(row['desc_tokens'], list):\n",
        "                tokens.extend(row['desc_tokens'])\n",
        "            if 'details_tokens' in row and isinstance(row['details_tokens'], list):\n",
        "                tokens.extend(row['details_tokens'])\n",
        "\n",
        "            # Calculate TF and document length\n",
        "            tf_counts = Counter(tokens)\n",
        "            L_d = math.sqrt(sum(tf ** 2 for tf in tf_counts.values()))\n",
        "            self.doc_lengths[pid] = L_d if L_d > 0 else 1.0\n",
        "\n",
        "            # Update postings\n",
        "            for term, tf in tf_counts.items():\n",
        "                if pid not in self.index[term]['postings']:\n",
        "                    self.index[term]['df'] += 1\n",
        "                self.index[term]['postings'][pid] = tf\n",
        "\n",
        "        self.num_docs = len(df)\n",
        "        print(f\"Index built: {self.num_docs} documents, {len(self.index)} unique terms.\")\n",
        "\n",
        "    def get_term_stats(self, term):\n",
        "        \"\"\"Retrieves statistics for a given term.\"\"\"\n",
        "        return self.index.get(term, {'df': 0, 'postings': {}})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc55445a-ef75-4d64-acdf-a604ee20fd5d",
      "metadata": {
        "id": "dc55445a-ef75-4d64-acdf-a604ee20fd5d"
      },
      "source": [
        "**TF-IDF Ranking and Retrieval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "365ea067-152e-4163-baf6-0a67b534ac1c",
      "metadata": {
        "id": "365ea067-152e-4163-baf6-0a67b534ac1c"
      },
      "outputs": [],
      "source": [
        "def calculate_tfidf_weight(tf, df, N):\n",
        "    \"\"\"Calculates TF-IDF weight using 1+log(tf) * log(N/df).\"\"\"\n",
        "    tf_comp = 1 + math.log10(tf) if tf > 0 else 0\n",
        "    idf_comp = math.log10(N / df) if df > 0 else 0\n",
        "    return tf_comp * idf_comp\n",
        "\n",
        "\n",
        "def ranked_search(query: str, index: InvertedIndex) -> List[Tuple[str, float]]:\n",
        "    \"\"\"Retrieves documents using AND logic and ranks with Cosine Similarity.\"\"\"\n",
        "    N = index.num_docs\n",
        "    query_tokens = build_terms(query)\n",
        "\n",
        "    if not query_tokens:\n",
        "        return []\n",
        "\n",
        "    # Strict AND intersection\n",
        "    doc_sets = []\n",
        "    for term in query_tokens:\n",
        "        term_stats = index.get_term_stats(term)\n",
        "        if term_stats['df'] == 0:\n",
        "            return []  # Term not in index, no results\n",
        "        doc_sets.append(set(term_stats['postings'].keys()))\n",
        "\n",
        "    retrieved_pids = list(set.intersection(*doc_sets)) if doc_sets else []\n",
        "\n",
        "    if not retrieved_pids:\n",
        "        return []\n",
        "\n",
        "    # Calculate scores\n",
        "    scores = {}\n",
        "    q_tf_counts = Counter(query_tokens)\n",
        "\n",
        "    for pid in retrieved_pids:\n",
        "        score = 0.0\n",
        "        L_d = index.doc_lengths.get(pid, 1.0)\n",
        "\n",
        "        for term in query_tokens:\n",
        "            term_stats = index.get_term_stats(term)\n",
        "            df = term_stats['df']\n",
        "            tf_d = term_stats['postings'].get(pid, 0)\n",
        "            tf_q = q_tf_counts[term]\n",
        "\n",
        "            W_t_d = calculate_tfidf_weight(tf_d, df, N)\n",
        "            W_t_q = calculate_tfidf_weight(tf_q, df, N)\n",
        "            score += W_t_q * W_t_d\n",
        "\n",
        "        scores[pid] = score / L_d\n",
        "\n",
        "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0c948ce-6584-4b9b-9b9b-3a097b5049ff",
      "metadata": {
        "id": "a0c948ce-6584-4b9b-9b9b-3a097b5049ff"
      },
      "source": [
        "**Evaluation Metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f31818f0-cabb-4033-bfaf-805e9aa7346b",
      "metadata": {
        "id": "f31818f0-cabb-4033-bfaf-805e9aa7346b"
      },
      "outputs": [],
      "source": [
        "def get_relevance_labels(query_id, retrieved_pids, df_labels):\n",
        "    \"\"\"Returns ranked binary relevance scores (1/0) and R_total.\"\"\"\n",
        "    relevant_df = df_labels[df_labels['query_id'] == query_id]\n",
        "    relevant_pids = set(relevant_df[relevant_df['relevance'] == 1]['pid'].astype(str))\n",
        "    R_total = len(relevant_pids)\n",
        "    relevance_scores = [1 if pid in relevant_pids else 0 for pid in retrieved_pids]\n",
        "    return relevance_scores, R_total\n",
        "\n",
        "\n",
        "# --- Evaluation Metrics ---\n",
        "def precision_at_k(rel_scores, k):\n",
        "    if k == 0 or not rel_scores: return 0.0\n",
        "    k = min(k, len(rel_scores))\n",
        "    return sum(rel_scores[:k]) / k\n",
        "\n",
        "def recall_at_k(rel_scores, R_total, k):\n",
        "    if R_total == 0: return 0.0\n",
        "    k = min(k, len(rel_scores))\n",
        "    return sum(rel_scores[:k]) / R_total\n",
        "\n",
        "def f1_score_at_k(P_at_k, R_at_k):\n",
        "    if P_at_k + R_at_k == 0: return 0.0\n",
        "    return 2 * P_at_k * R_at_k / (P_at_k + R_at_k)\n",
        "\n",
        "def average_precision_at_k(rel_scores, k):\n",
        "    if not rel_scores: return 0.0\n",
        "    k = min(k, len(rel_scores))\n",
        "    sum_precisions = 0.0\n",
        "    num_relevant = 0\n",
        "    for i in range(k):\n",
        "        if rel_scores[i] == 1:\n",
        "            num_relevant += 1\n",
        "            sum_precisions += num_relevant / (i + 1)\n",
        "    return sum_precisions / num_relevant if num_relevant > 0 else 0.0\n",
        "\n",
        "def mean_average_precision(aps_list):\n",
        "    return sum(aps_list) / len(aps_list) if aps_list else 0.0\n",
        "\n",
        "def mean_reciprocal_rank(rel_scores):\n",
        "    for i, rel in enumerate(rel_scores):\n",
        "        if rel == 1:\n",
        "            return 1.0 / (i + 1)\n",
        "    return 0.0\n",
        "\n",
        "def ndcg_at_k(rel_scores, k):\n",
        "    k = min(k, len(rel_scores))\n",
        "    dcg = sum(rel_scores[i] / math.log2(i + 2) for i in range(k))\n",
        "    ideal_scores = sorted(rel_scores, reverse=True)\n",
        "    idcg = sum(ideal_scores[i] / math.log2(i + 2) for i in range(k))\n",
        "    return dcg / idcg if idcg > 0.0 else 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a6129b-2b5f-47a7-b193-f9bf60b1d839",
      "metadata": {
        "id": "e3a6129b-2b5f-47a7-b193-f9bf60b1d839"
      },
      "source": [
        "**Execution - Build Index and Define Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f186a20-6376-465a-a102-cc4c8ddda89d",
      "metadata": {
        "id": "6f186a20-6376-465a-a102-cc4c8ddda89d",
        "outputId": "1dd9a46d-b2ef-484a-ecfa-ab5fbe5fdfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project root: /content\n",
            "Looking for data at: /content/data/processed/products_clean.parquet\n",
            "ERROR: Processed data not found at /content/data/processed/products_clean.parquet\n",
            "Run Part 1 preprocessing first!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        }
      ],
      "source": [
        "current_dir = Path.cwd()\n",
        "PROJECT_ROOT = current_dir\n",
        "\n",
        "# Look for 'data' folder in current or parent directories\n",
        "if not (PROJECT_ROOT / 'data').exists():\n",
        "    if (PROJECT_ROOT.parent / 'data').exists():\n",
        "        PROJECT_ROOT = PROJECT_ROOT.parent\n",
        "    else:\n",
        "        print(\"ERROR: Cannot find 'data' directory. Ensure you're running from project root.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "PROCESSED_DATA_FILE = PROJECT_ROOT / 'data' / 'processed' / 'products_clean.parquet'\n",
        "LABELS_FILE = PROJECT_ROOT / 'data' / 'raw' / 'validation_labels.csv'\n",
        "INDEX_DIR = PROJECT_ROOT / 'data' / 'index'\n",
        "INDEX_FILE = INDEX_DIR / 'inverted_index.json'\n",
        "\n",
        "# Create index directory if it doesn't exist\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Looking for data at: {PROCESSED_DATA_FILE}\")\n",
        "\n",
        "# --- Load Data ---\n",
        "if not PROCESSED_DATA_FILE.exists():\n",
        "    print(f\"ERROR: Processed data not found at {PROCESSED_DATA_FILE}\")\n",
        "    print(\"Run Part 1 preprocessing first!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"\\n--- LOADING DATA ---\")\n",
        "df_clean = pd.read_parquet(PROCESSED_DATA_FILE)\n",
        "print(f\"Loaded {len(df_clean)} products\")\n",
        "print(f\"Columns: {list(df_clean.columns)}\")\n",
        "\n",
        "# Load labels\n",
        "if not LABELS_FILE.exists():\n",
        "    print(f\"ERROR: Labels file not found at {LABELS_FILE}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "df_labels = pd.read_csv(LABELS_FILE)\n",
        "print(f\"Loaded {len(df_labels)} relevance labels\")\n",
        "\n",
        "# --- Build Index ---\n",
        "print(\"\\n--- BUILDING INVERTED INDEX ---\")\n",
        "INDEX = InvertedIndex()\n",
        "INDEX.build_from_dataframe(df_clean)\n",
        "\n",
        "# Save index\n",
        "print(f\"\\nSaving index to {INDEX_FILE}...\")\n",
        "with open(INDEX_FILE, 'w') as f:\n",
        "    json.dump({\n",
        "        'index': {k: dict(v) for k, v in INDEX.index.items()},\n",
        "        'doc_lengths': INDEX.doc_lengths,\n",
        "        'num_docs': INDEX.num_docs\n",
        "    }, f)\n",
        "print(\"Index saved successfully!\")\n",
        "\n",
        "# --- Define Queries ---\n",
        "ALL_QUERIES = [\n",
        "    {'query_id': 1, 'query': 'women full sleeve sweatshirt cotton'},\n",
        "    {'query_id': 2, 'query': 'men slim jeans blue'},\n",
        "    {'query_id': 3, 'query': 'long sleeve denim jacket blue'},\n",
        "    {'query_id': 4, 'query': 'reeb shoe sport white'},\n",
        "    {'query_id': 5, 'query': 'cheap men polo shirt black'},\n",
        "    {'query_id': 6, 'query': 'tight fit short skirt women'},\n",
        "    {'query_id': 7, 'query': 'low price formal trouser'},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc67f28-3268-49d7-b24f-e67efef17373",
      "metadata": {
        "id": "2bc67f28-3268-49d7-b24f-e67efef17373"
      },
      "source": [
        "**Evaluation 1 & 2 Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b503d69-eba3-4a1a-8faf-76aa23d08888",
      "metadata": {
        "id": "6b503d69-eba3-4a1a-8faf-76aa23d08888"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- RUNNING EVALUATION ---\")\n",
        "RESULTS = []\n",
        "AP_SCORES = []\n",
        "\n",
        "for q_info in ALL_QUERIES:\n",
        "    query_id = q_info['query_id']\n",
        "    query_text = q_info['query']\n",
        "\n",
        "    print(f\"\\nQuery {query_id}: '{query_text}'\")\n",
        "\n",
        "    # Search\n",
        "    ranked_results = ranked_search(query_text, INDEX)\n",
        "    retrieved_pids = [pid for pid, score in ranked_results]\n",
        "\n",
        "    print(f\"  Retrieved {len(retrieved_pids)} documents\")\n",
        "\n",
        "    # Get relevance labels\n",
        "    relevance_scores, R_total = get_relevance_labels(query_id, retrieved_pids, df_labels)\n",
        "\n",
        "    # Calculate metrics\n",
        "    k = 10\n",
        "    P_k = precision_at_k(relevance_scores, k)\n",
        "    R_k = recall_at_k(relevance_scores, R_total, k)\n",
        "    F1_k = f1_score_at_k(P_k, R_k)\n",
        "    AP_k = average_precision_at_k(relevance_scores, k)\n",
        "    MRR_score = mean_reciprocal_rank(relevance_scores)\n",
        "    NDCG_k = ndcg_at_k(relevance_scores, k)\n",
        "\n",
        "    AP_SCORES.append(AP_k)\n",
        "\n",
        "    RESULTS.append({\n",
        "        'Query ID': query_id,\n",
        "        'Query': query_text,\n",
        "        'R_Total': R_total,\n",
        "        'Retrieved': len(retrieved_pids),\n",
        "        'P@10': round(P_k, 3),\n",
        "        'R@10': round(R_k, 3),\n",
        "        'F1@10': round(F1_k, 3),\n",
        "        'AP@10': round(AP_k, 3),\n",
        "        'MRR': round(MRR_score, 3),\n",
        "        'NDCG@10': round(NDCG_k, 3),\n",
        "    })\n",
        "\n",
        "# --- Display Results ---\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*80)\n",
        "df_results = pd.DataFrame(RESULTS)\n",
        "print(df_results.to_string(index=False))\n",
        "\n",
        "MAP_score = mean_average_precision(AP_SCORES)\n",
        "print(f\"\\nMean Average Precision (MAP): {round(MAP_score, 3)}\")\n",
        "\n",
        "# Save results\n",
        "results_file = PROJECT_ROOT / 'data' / 'results' / 'evaluation_results.csv'\n",
        "results_file.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_results.to_csv(results_file, index=False)\n",
        "print(f\"\\nResults saved to {results_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a05322ca-33ba-45d0-9741-8c10da32df65",
      "metadata": {
        "id": "a05322ca-33ba-45d0-9741-8c10da32df65"
      },
      "source": [
        "**Ground Truth for New Queries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bd4f48e-33e8-498a-83eb-339c20d00770",
      "metadata": {
        "id": "4bd4f48e-33e8-498a-83eb-339c20d00770"
      },
      "outputs": [],
      "source": [
        "\n",
        "# The assignment requires you to manually define the ground truth for Q3-Q7\n",
        "# and update your validation_labels.csv file.\n",
        "\n",
        "# ACTION REQUIRED:\n",
        "# 1. Inspect the PIDs retrieved by the search engine for Queries 3 through 7.\n",
        "# 2. For those PIDs, manually judge relevance (1 or 0).\n",
        "# 3. Add these new relevance judgments to your data/raw/validation_labels.csv file.\n",
        "# 4. Include a detailed table of your manual judgments in your final PDF report.\n",
        "\n",
        "# Example:\n",
        "# Query 3: 'long sleeve denim jacket blue'\n",
        "# - PID_12345: Relevant (1) because it is a denim jacket.\n",
        "# - PID_67890: Not Relevant (0) because it is a denim dress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80dde80-7f03-4fd0-aee0-a30424a66ce1",
      "metadata": {
        "id": "f80dde80-7f03-4fd0-aee0-a30424a66ce1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}